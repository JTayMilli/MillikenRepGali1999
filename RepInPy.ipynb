{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "from scipy.stats import norm\n",
    "from numpy.linalg import inv, cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svarlr import varlr, irflr, cor, bootstrap, band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_excel('DATA.xlsx').rename(columns = {'Unnamed: 0' : \"Time\"})\n",
    "data = DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'US'\n",
    "\n",
    "nchoice = 2\n",
    "#(1) n: employment\n",
    "#(2) n: hours\n",
    "\n",
    "nint = 1\n",
    "#(1) n=I(1)\n",
    "#(0) n=I(0)\n",
    "\n",
    "difn = 'yes'\n",
    "#'yes' computed correlations based on employment growth\n",
    "#'no' computed correlations based on detrended employment\n",
    "\n",
    "LAGS=4\n",
    "NVAR=2\n",
    "NSTEP= 100\n",
    "NDRAWS= 500\n",
    "NSE = 2\n",
    "IRH = 13\n",
    "NW = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nchoice==1: labor = 'LHEM' #'employment'\n",
    "elif nchoice==2: labor = 'LPMHU' #'hours'\n",
    "gdp = 'GDPQ'\n",
    "labor_series = data[labor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"country\")\n",
    "print('n: \\t {labor} I({nint})')\n",
    "\n",
    "yx = DATA[gdp]\n",
    "nber = DATA.NBER #Manually entered recession start and end times according to nber\n",
    "nberg = DATA.NBERG\n",
    "\n",
    "nx = DATA[labor]\n",
    "\n",
    "nobs = len(yx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AR test stuff\n",
    "xx = yx/nx\n",
    "xx[0:4] = np.nan\n",
    "xx.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_29572\\2407165987.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['n'][0:4] = np.nan\n",
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_29572\\2407165987.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['dn'][0:5] = np.nan\n",
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_29572\\2407165987.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['y'][0:4] = np.nan\n",
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_29572\\2407165987.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['dy'][0:5] = np.nan\n",
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_29572\\2407165987.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['x'][0:4] = np.nan\n",
      "C:\\Users\\joshu\\AppData\\Local\\Temp\\ipykernel_29572\\2407165987.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['dx'][0:5] = np.nan\n"
     ]
    }
   ],
   "source": [
    "data['dlabor'] = np.log(data[labor]) - np.log(data[labor].shift(1))\n",
    "data['n'] = 100 + np.log(data[labor]/data[labor][4]) * 100\n",
    "data['n'][0:4] = np.nan\n",
    "data['dn'] = data['dlabor']*100\n",
    "data['dn'][0:5] = np.nan\n",
    "\n",
    "data['dgdp'] = np.log(data[gdp]) - np.log(data[gdp].shift(1))\n",
    "data['y'] = 100 + np.log(data[gdp]/data[gdp][4]) * 100\n",
    "data['y'][0:4] = np.nan\n",
    "data['dy'] = data['dgdp']*100\n",
    "data['dy'][0:5] = np.nan\n",
    "\n",
    "data['xx'] = data[gdp].div(data[labor])\n",
    "data['x'] = 100 + np.log(data['xx']/data['xx'][4]) * 100\n",
    "data['x'][0:4] = np.nan\n",
    "data['dx'] = (np.log(data['xx']) - np.log(data['xx'].shift(1)))*100\n",
    "data['dx'][0:5] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Add a constant term for the intercept\n",
    "ser = data.n.dropna()\n",
    "X = sm.add_constant(np.arange(0,len(ser)))\n",
    "model = sm.OLS(np.array(ser), X)\n",
    "result = model.fit()\n",
    "\n",
    "# Get the linear trend\n",
    "trend = result.predict(X)\n",
    "\n",
    "# Detrend the series\n",
    "detrended_series = ser - trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.nan] * 4 + detrended_series.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation coefficeint\n",
    "np.corrcoef(data.dx.dropna(),data.dn.dropna())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the VAR model on dy, dn, and dx\n",
    "vard = data[['dy', 'dn', 'dx']].dropna()\n",
    "model = VAR(vard)\n",
    "var_results = model.fit(maxlags=4)  # Use maxlags similar to the RATS code, adjust as needed\n",
    "\n",
    "# Check the summary\n",
    "print(var_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HP filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HP filter to GDP and labor data\n",
    "y_cycle, y_trend = hpfilter(data.y.dropna(), lamb=1600)\n",
    "n_cycle, n_trend = hpfilter(data.n.dropna(), lamb=1600)\n",
    "x_cycle, x_trend = hpfilter(data.x.dropna(), lamb=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x_cycle,n_cycle)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdata = data[['y', 'n', 'x']].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAR(vdata)\n",
    "var_results = model.fit(maxlags=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from numpy.linalg import cholesky\n",
    "\n",
    "\n",
    "# Assuming 'data' contains the variables 'dx' and 'dn'\n",
    "# 'lags' is the number of lags in the VAR model\n",
    "varD = data[['dx', 'dn']].dropna()\n",
    "model = VAR(varD)\n",
    "var_results = model.fit(maxlags=4)  # Replace with the appropriate number of lags\n",
    "residuals = var_results.resid  # Residuals are needed for shocks\n",
    "\n",
    "# Get covariance of the residuals\n",
    "sigmatR = var_results.sigma_u\n",
    "sigmat = sigmatR.copy()\n",
    "sigmat['dx'] = [0.43114, -0.06029]\n",
    "sigmat['dn'] = [np.nan, 0.42170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'sigmat' is the covariance matrix of the residuals from the VAR model\n",
    "\n",
    "\n",
    "##### NEED TO SOLVE\n",
    "emats = np.array([[0.88916, -0.49740], [0.86501, 1.73441]])\n",
    "\n",
    "# Cholesky decomposition to get the lower triangular matrix\n",
    "cmats = emats*np.array(sigmat)*emats.T\n",
    "\n",
    "# Equivalent to RATS `smat = inv(emats)*cmats` assuming emats is the identity matrix\n",
    "smat = cmats  # This is already a lower triangular matrix\n",
    "\n",
    "# Invert smat to get sinv (similar to RATS code)\n",
    "sinv = inv(smat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cholesky(np.array(sigmat)))\n",
    "print(emats.T)\n",
    "print(cmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.array([[0.58541, .29737],[-.37301, 0.53157]])\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varlrMilliken(series, p=1):\n",
    "    (nobs, nvar) = series.shape\n",
    "    series = np.asarray(series)\n",
    "    YY = series[p:nobs]\n",
    "    XX = np.ones([nobs - p, 1 + nvar * p])\n",
    "    \n",
    "    for i in range(1, p + 1):\n",
    "        start_col = 1 + nvar * (i - 1)\n",
    "        end_col = 1 + nvar * i\n",
    "        XX[:, start_col:end_col] = series[p - i:nobs - i, :]\n",
    "    \n",
    "    # Estimate VAR coefficients\n",
    "    b = np.linalg.lstsq(XX, YY, rcond=None)[0]\n",
    "    e = YY - np.dot(XX, b)\n",
    "    Sig = np.dot(e.conj().transpose(), e) / (nobs - p)\n",
    "    \n",
    "    # Extract AR Coefficient Matrices\n",
    "    AR_matrices = []\n",
    "    for i in range(p):\n",
    "        start_idx = 1 + i * nvar\n",
    "        end_idx = 1 + (i + 1) * nvar\n",
    "        Ai = b[start_idx:end_idx, :].conj().transpose()\n",
    "        AR_matrices.append(Ai)\n",
    "    \n",
    "    # Sum of AR Coefficients\n",
    "    A_sum = np.zeros((nvar, nvar))\n",
    "    for Ai in AR_matrices:\n",
    "        A_sum += Ai\n",
    "    \n",
    "    # Compute Phi1\n",
    "    Phi1 = np.linalg.inv(np.identity(nvar) - A_sum)\n",
    "    \n",
    "    # Compute mu and lvar\n",
    "    mu = np.dot(Phi1, b[0].conj().transpose())\n",
    "    lvar = np.dot(Phi1, np.dot(Sig, Phi1.conj().transpose()))\n",
    "    \n",
    "    # Long-Run restriction\n",
    "    theta1 = np.linalg.cholesky(lvar.conj().transpose())\n",
    "    B = np.dot(np.linalg.inv(Phi1), theta1)\n",
    "    \n",
    "    return (b, Sig, B, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.svar_model import SVAR\n",
    "# Now, we set up the matrices needed for identification\n",
    "A = np.array([['E', 0],\n",
    "              ['E', 'E']])\n",
    "\n",
    "# We can estimate the SVAR with the long-run restriction using statsmodels\n",
    "svar_model = SVAR(varD, svar_type='A', A=A, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the long-run restriction matrix\n",
    "A_longrun = np.array([['E', 0],\n",
    "                      ['E', 'E']])\n",
    "\n",
    "# Estimate the SVAR model with long-run restrictions\n",
    "svar_model = SVAR(varD, svar_type='A', A=A_longrun)\n",
    "svar_results = svar_model.fit(maxlags = LAGS)\n",
    "\n",
    "# Step 8: Impulse Responses\n",
    "# Compute impulse responses up to NSTEP periods\n",
    "irf = svar_results.irf(NSTEP)\n",
    "irfs = irf.irfs  # Shape: (NSTEP+1, nvars, nvars)\n",
    "\n",
    "# Extract the impulse responses\n",
    "c11 = irfs[:, 0, 0]  # Response of 'dx' to technology shock\n",
    "c12 = irfs[:, 0, 1]  # Response of 'dx' to non-technology shock\n",
    "c21 = irfs[:, 1, 0]  # Response of 'dnz' to technology shock\n",
    "c22 = irfs[:, 1, 1]  # Response of 'dnz' to non-technology shock\n",
    "\n",
    "# Adjust 'c21' and 'c22' if 'nint' == 0 and 'difn' == 'yes'\n",
    "if nint == 0 and difn == 'yes':\n",
    "    c21_adjusted = np.concatenate(([c21[0]], np.diff(c21)))\n",
    "    c22_adjusted = np.concatenate(([c22[0]], np.diff(c22)))\n",
    "    c21 = c21_adjusted\n",
    "    c22 = c22_adjusted\n",
    "\n",
    "# Compute cumulative impulse responses\n",
    "c11s = np.cumsum(c11)\n",
    "c12s = np.cumsum(c12)\n",
    "c21s = np.cumsum(c21)\n",
    "c22s = np.cumsum(c22)\n",
    "\n",
    "# Step 9: Historical Decomposition\n",
    "# Compute the structural shocks\n",
    "eps = svar_results.resid  # Structural shocks: [eps1, eps2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, Sig, B, e = varlrMilliken(series = varD, p=4)\n",
    "print(b)\n",
    "print(f\"Sig {Sig}\")\n",
    "print(B)\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "irfs = irflr(b,B,IRH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Compute conditional variances and covariances using MA coefficients\n",
    "# For productivity ('dx') and hours ('dnz')\n",
    "\n",
    "# Extract MA coefficients for 'dx' and 'dnz' responses to each shock\n",
    "# Let's denote:\n",
    "# - C_11: response of 'dx' to technology shock\n",
    "# - C_12: response of 'dx' to non-technology shock\n",
    "# - C_21: response of 'dnz' to technology shock\n",
    "# - C_22: response of 'dnz' to non-technology shock\n",
    "\n",
    "C_11 = irfs[0]  # 'dx' response to tech shock\n",
    "C_12 = irfs[2]  # 'dx' response to non-tech shock\n",
    "C_21 = irfs[1]  # 'dnz' response to tech shock\n",
    "C_22 = irfs[3]  # 'dnz' response to non-tech shock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(C_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = pd.read_excel(\"Cs.xlsx\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "C_11 = Cs.c11  # 'dx' response to tech shock\n",
    "C_12 = Cs.c12  # 'dx' response to non-tech shock\n",
    "C_21 = Cs.c21  # 'dnz' response to tech shock\n",
    "C_22 = Cs.c22 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_var_contribution(AR_matrices, B, eps_shock, p, initial_values):\n",
    "    T = eps_shock.shape[0]\n",
    "    nvar = B.shape[0]\n",
    "    Y_sim = np.zeros((T, nvar))\n",
    "    Y_sim[:p, :] = initial_values\n",
    "    for t in range(p, T):\n",
    "        Y_t = np.zeros(nvar)\n",
    "        for i in range(p):\n",
    "            Y_t += AR_matrices[i] @ Y_sim[t - i - 1, :]\n",
    "        Y_t += B @ eps_shock[t, :]\n",
    "        Y_sim[t, :] = Y_t\n",
    "    return Y_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrend = 0\n",
    "corr_uncon = np.corrcoef(data.dx.dropna(), data.dn.dropna())[0,1]\n",
    "varD = data[['dx', 'dn']].dropna()\n",
    "b, Sig, B, e = varlrMilliken(series=varD, p=4)\n",
    "irfs = irflr(b, B, IRH)\n",
    "# Existing code to compute impulse responses...\n",
    "\n",
    "# Compute structural shocks\n",
    "eps = np.linalg.solve(B, e.T).T  # Shape: (T, nvar)\n",
    "\n",
    "# Extract VAR coefficient matrices\n",
    "p = 4  # Number of lags\n",
    "nvar = 2  # Number of variables\n",
    "AR_matrices = []\n",
    "for i in range(p):\n",
    "    start_idx = 1 + i * nvar\n",
    "    end_idx = 1 + (i + 1) * nvar\n",
    "    Ai = b[start_idx:end_idx, :].T\n",
    "    AR_matrices.append(Ai)\n",
    "\n",
    "# Initial values\n",
    "initial_values = varD.values[:p, :]\n",
    "\n",
    "# Simulate contributions from each shock\n",
    "contributions = []\n",
    "for j in range(nvar):\n",
    "    eps_shock = np.zeros_like(eps)\n",
    "    eps_shock[:, j] = eps[:, j]\n",
    "    Y_sim = simulate_var_contribution(AR_matrices, B, eps_shock, p, initial_values)\n",
    "    contributions.append(Y_sim)\n",
    "contributions = np.stack(contributions, axis=2)  # (T, nvar, nvar)\n",
    "\n",
    "# Extract contributions\n",
    "hd11 = contributions[:, 0, 0]\n",
    "hd12 = contributions[:, 1, 0]\n",
    "hd21z = contributions[:, 0, 1]\n",
    "hd22z = contributions[:, 1, 1]\n",
    "\n",
    "# Adjust contributions based on detrending\n",
    "if detrend == 1:\n",
    "    hd21 = np.zeros_like(hd21z)\n",
    "    hd22 = np.zeros_like(hd22z)\n",
    "    hd21[0] = hd21z[0]\n",
    "    hd22[0] = hd22z[0]\n",
    "    hd21[1:] = hd21z[1:] - hd21z[:-1]\n",
    "    hd22[1:] = hd22z[1:] - hd22z[:-1]\n",
    "else:\n",
    "    hd21 = hd21z\n",
    "    hd22 = hd22z\n",
    "\n",
    "# Compute total contributions\n",
    "hd31 = hd21 + hd11\n",
    "hd32 = hd22 + hd12\n",
    "\n",
    "# Assign contributions to variables\n",
    "dyt = hd31\n",
    "dnt = hd21\n",
    "dxt = hd11\n",
    "dyd = hd32\n",
    "dnd = hd22\n",
    "dxd = hd12\n",
    "\n",
    "# Compute cumulative sums\n",
    "yt = np.cumsum(dyt)\n",
    "yd = np.cumsum(dyd)\n",
    "xt = np.cumsum(dxt)\n",
    "xd = np.cumsum(dxd)\n",
    "nt = np.cumsum(dnt)\n",
    "nd = np.cumsum(dnd)\n",
    "\n",
    "# Adjust nt and nd if needed\n",
    "if detrend == 1:\n",
    "    nt = dnt\n",
    "    nd = dnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07160785e+00, -3.91779933e-01,  1.49111893e+00,  1.01564341e+00,\n",
       "        9.58147109e-01,  9.85452753e-01, -2.45031098e-01,  4.48522037e-02,\n",
       "       -9.04903747e-04,  3.65364630e-02, -6.92160863e-01, -2.50050474e-01,\n",
       "       -4.85105048e-01,  4.25448657e-01,  6.17305351e-01,  4.06366021e-02,\n",
       "       -2.47779498e-01, -5.33461484e-01, -1.87557306e-01, -1.73875626e-01,\n",
       "        3.82859289e-01,  2.56488726e-01,  4.99477721e-01,  1.86080662e-01,\n",
       "       -1.20260754e-01, -3.70048411e-01,  1.52862762e-01, -4.75025057e-01,\n",
       "        2.17162715e-01, -5.13967438e-02,  5.46121313e-01, -4.98408489e-01,\n",
       "       -1.31779119e-01,  6.10550505e-02, -6.54143257e-01, -4.88197197e-01,\n",
       "        7.16470214e-01,  1.02774629e+00,  8.83309301e-02, -9.08994291e-02,\n",
       "       -7.06467792e-02, -7.99971095e-01,  3.95620375e-01,  3.58022208e-01,\n",
       "       -5.17159755e-01, -1.45499925e-01, -2.88696874e-01,  4.36644625e-01,\n",
       "        3.75322904e-01,  8.86521966e-02,  9.23580112e-02, -2.35585228e-01,\n",
       "        7.36885447e-03, -3.54726960e-01, -1.03082622e-01,  2.89780052e-01,\n",
       "        3.46498104e-01, -5.38932030e-02, -1.49951773e-01,  1.82350428e-01,\n",
       "       -1.02744275e-01, -3.50315455e-01,  3.11838475e-02,  3.04455037e-01,\n",
       "        7.85005399e-02,  1.48065368e-01,  2.66022211e-01, -5.05630134e-02,\n",
       "       -7.25703276e-01, -9.75738007e-03, -2.97184934e-02, -2.09961099e-02,\n",
       "        3.93039383e-02,  2.92870748e-01, -1.44229934e-01,  1.46310514e-01,\n",
       "        1.65446439e-01, -2.40793025e-01, -3.31825550e-01,  4.35510984e-01,\n",
       "       -3.15440664e-01,  8.73275751e-02, -2.22596182e-01, -7.62078662e-02,\n",
       "        9.29868499e-03,  2.71723136e-01, -1.56345104e-01,  3.71413550e-01,\n",
       "       -2.17982402e-01, -2.73446388e-01,  1.90173064e-01,  4.89554128e-01,\n",
       "        4.28225947e-02, -1.37202336e-01,  9.42718096e-02,  2.34951990e-01,\n",
       "       -5.58893916e-01, -4.19885181e-01,  2.06186315e-01, -3.32148431e-01,\n",
       "        1.93945881e-01,  5.20225221e-02, -2.13656843e-01, -4.62405835e-01,\n",
       "        9.48040263e-01,  5.32549526e-01, -1.20143900e-01,  1.54508341e-02,\n",
       "       -5.98660833e-01, -7.21142068e-02,  1.69834114e-01,  2.80312429e-01,\n",
       "        2.44234979e-01, -1.65522076e-01, -3.98961120e-01, -2.27417260e-02,\n",
       "        1.10021712e+00, -7.58082784e-01,  1.41057298e-01, -5.54213997e-01,\n",
       "       -5.67900364e-02,  2.16257335e-01, -1.48468978e-01,  1.49276841e-01,\n",
       "       -1.02165182e+00,  6.65090941e-01,  9.16905796e-01, -9.01225065e-02,\n",
       "       -6.03106748e-01,  1.98864689e-03, -6.20102244e-01, -1.70348553e-01,\n",
       "        6.56207085e-01, -1.28271285e-01,  2.44174234e-01,  2.49598035e-01,\n",
       "        5.95678796e-01, -1.31991274e-01, -1.45759400e-01,  4.31005542e-02,\n",
       "       -2.41529702e-01, -2.42679379e-01, -5.25672947e-02,  5.96688734e-02,\n",
       "        6.42079778e-02,  1.85564640e-01, -8.06723422e-02, -2.68207570e-02,\n",
       "       -2.73599595e-01,  2.32115237e-02, -2.25088196e-03,  1.83280570e-01,\n",
       "        1.45757084e-01,  8.91041805e-02,  6.16217507e-02, -1.73119954e-01,\n",
       "       -1.86269227e-02, -1.68267603e-01,  1.04806891e-01, -3.28876136e-02,\n",
       "       -1.33496295e-01, -1.18031704e-01,  1.37631386e-01,  1.12504559e-01,\n",
       "        2.52434628e-02, -3.39243797e-01, -2.63426851e-01,  3.21267231e-02,\n",
       "        3.64410775e-01,  5.33046614e-02, -1.05437148e-01,  1.70422695e-01,\n",
       "        1.57334276e-02,  6.89631276e-02,  1.82599340e-01, -3.04792366e-01,\n",
       "       -9.93396283e-03,  6.15993895e-03,  3.20898298e-01, -9.74704914e-02,\n",
       "        7.86708161e-02, -1.59917299e-01,  2.02969323e-01])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_11s = np.cumsum(C_11)\n",
    "C_12s = np.cumsum(C_12)\n",
    "C_21s = np.cumsum(C_21)\n",
    "C_22s = np.cumsum(C_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variances of 'dx' and 'dnz' for each component\n",
    "# Variance due to technology shock\n",
    "var_dx_tech = np.sum(C_11 ** 2)\n",
    "var_dnz_tech = np.sum(C_21 ** 2)\n",
    "cov_dx_dnz_tech = np.sum(C_11 * C_21)\n",
    "\n",
    "# Variance due to non-technology shock\n",
    "var_dx_nontech = np.sum(C_12 ** 2)\n",
    "var_dnz_nontech = np.sum(C_22 ** 2)\n",
    "cov_dx_dnz_nontech = np.sum(C_12 * C_22)\n",
    "\n",
    "# Compute conditional correlations\n",
    "corr_tech = cov_dx_dnz_tech / np.sqrt(var_dx_tech * var_dnz_tech)\n",
    "corr_nontech = cov_dx_dnz_nontech / np.sqrt(var_dx_nontech * var_dnz_nontech)\n",
    "\n",
    "print(\"Conditional Correlation (Technology Shock):\", corr_tech)\n",
    "print(\"Conditional Correlation (Non-Technology Shock):\", corr_nontech)\n",
    "\n",
    "# Step 10: Generate Figure 1 - Scatter plots of innovations\n",
    "# Obtain structural shocks (innovations)\n",
    "#epsilon = svar_results.resid\n",
    "\n",
    "# Note: Since the shocks are already estimated, we can use them directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([C_11,C_21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(\n",
    "    data=data[['dn','dx']].dropna(), \n",
    "    x='dn', \n",
    "    y='dx', \n",
    "    marker='s',        # 's' is the marker style for squares\n",
    "    color='black',     # Set color to black\n",
    "    edgecolor='black'  # Set edge color to black (useful if the color of the marker is different)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Hours', fontsize=14)\n",
    "plt.ylabel('Productivity', fontsize=14)\n",
    "plt.title('Data', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot impulse responses to technology shock\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "axes[0].plot(C_11s[:IRH], label='dx response to Technology Shock')\n",
    "axes[0].set_title('Impulse Response of dx to Technology Shock')\n",
    "axes[0].set_xlabel('Periods')\n",
    "axes[0].set_ylabel('Response')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(C_21s[:IRH], label='dnz response to Technology Shock')\n",
    "axes[1].set_title('Impulse Response of dnz to Technology Shock')\n",
    "axes[1].set_xlabel('Periods')\n",
    "axes[1].set_ylabel('Response')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot((C_11s + C_21s)[:IRH], label='(dx + dnz) response to Technology Shock')\n",
    "axes[2].set_title('Impulse Response of (dx + dnz) to Technology Shock')\n",
    "axes[2].set_xlabel('Periods')\n",
    "axes[2].set_ylabel('Response')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot impulse responses to non-technology shock\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "axes[0].plot(C_12s[:IRH], label='dx response to Non-Technology Shock')\n",
    "axes[0].set_title('Impulse Response of dx to Non-Technology Shock')\n",
    "axes[0].set_xlabel('Periods')\n",
    "axes[0].set_ylabel('Response')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(C_22s[:IRH], label='dnz response to Non-Technology Shock')\n",
    "axes[1].set_title('Impulse Response of dnz to Non-Technology Shock')\n",
    "axes[1].set_xlabel('Periods')\n",
    "axes[1].set_ylabel('Response')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot((C_12s + C_22s)[:IRH], label='(dx + dnz) response to Non-Technology Shock')\n",
    "axes[2].set_title('Impulse Response of (dx + dnz) to Non-Technology Shock')\n",
    "axes[2].set_xlabel('Periods')\n",
    "axes[2].set_ylabel('Response')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
